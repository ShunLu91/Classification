import numpy as np


a = [i for i in range(10)]


# plot
import matplotlib.pyplot as plt
print(np.arange(1, len(a)+1))


train_batch_loss_list = [2.4084071158399487, 0.34541576990802536, 0.31645489248549735, 0.1633709804258441, 0.23742900499564254, 0.5537248548108449, 0.26253755752943925, 0.17064689113025894, 0.125098958247699, 0.15491896407857955, 0.24940207635030748, 0.2461865387433264, 0.1350564905163641, 0.11915400403493025, 0.2012492818832831, 0.12502795180818685, 0.15548836034017072, 0.09576733481141683, 0.20379834727737528, 0.13054072224979898, 0.10052320576847112, 0.1203590021417054, 0.03793222216303803, 0.14749632516675568, 0.2141698125349535, 0.1319866301920585, 0.08520088143554937, 0.07275112090007133, 0.1097315184485407, 0.17728234726785458, 0.15828307724655058, 0.10697460664734215, 0.1041687169313167, 0.14027589428343618, 0.07955919051579394, 0.1214315328956707, 0.055885148295394274, 0.16011932581697658, 0.09083829115136874, 0.0832455177089801, 0.08734869048580343, 0.04094400231827093, 0.12811872255066514, 0.1649999555249322, 0.10962214231100642, 0.06948925427043195, 0.05691564082404852, 0.07932225599690203, 0.1461142658991139, 0.13565877739685026, 0.09462365847423392, 0.10028561671183311, 0.12644883438887664, 0.06098902957099904, 0.11465360845690276, 0.0449233490138363, 0.13585388644116478, 0.08589031935548255, 0.077868663390021, 0.07720235378788322, 0.04186122201179792, 0.10379491813167191, 0.14989567279338176, 0.10840414713063526, 0.05954949968902984, 0.046216021231509974, 0.061834981920840824, 0.13793253453243434, 0.11426090724284449, 0.08390778503724089, 0.09241672107391986, 0.11913856584353431, 0.052944222082256846, 0.11010076124316008, 0.04156032394672979, 0.1158925968828713, 0.0761800884351377, 0.07177732272886325, 0.07099565376862652, 0.03952339049930375, 0.09367374643105052, 0.14039322941297241, 0.10983028329077604, 0.055145653930669426, 0.03987893712820421, 0.053924655380469776, 0.13707123301594676, 0.10762827241507489, 0.07836781863395803, 0.08517045393051234, 0.11117413998201148, 0.04750960157823758, 0.10257005164901407, 0.04204839288405765, 0.10747529502417286, 0.06833146999431661, 0.05172111407704868, 0.06704100874022621, 0.03370834775037235, 0.07293330694129053, 0.1276029903439392, 0.10229573584552473, 0.05706107113950806, 0.03639626991682398, 0.05073760030957371, 0.11661487792591387, 0.10086651019681689, 0.07577177665235754, 0.08081762830771295, 0.10695279117994888, 0.04531859005115335, 0.09839782119170105, 0.041657145121903154, 0.10619598132305033, 0.06416154102944895, 0.054581775335261755, 0.06305717811707921, 0.03204914124908946, 0.07706222539875529, 0.12502834752199313, 0.09976923918102792, 0.05709157469714637, 0.033656168533879566, 0.04487861336103517, 0.11833072096072786, 0.09886801246300762, 0.07205560891540305, 0.07701698662034036, 0.10198304417390483, 0.04193993850551121, 0.09778013529530562, 0.04073608404658856, 0.102743483936286, 0.06851482217000343, 0.051617325658565666, 0.062225510176339434, 0.029290368107811653, 0.07799255966623289, 0.12295738323419224, 0.09824882788824758, 0.054339513494293765, 0.031080424743720112, 0.04658283753412579, 0.12082401438333482, 0.09798137056789664, 0.06882768118217572, 0.0766779516951009, 0.09823548329926106, 0.044432001852818044, 0.09996986298704161, 0.041903883035683306, 0.10207266196951671, 0.06198737137060258, 0.04838646754558943, 0.057501565094307046, 0.027735738565122736, 0.08927329571172686, 0.1178386912764202, 0.09836028270878065, 0.05088656487754195, 0.029184154748159843, 0.043107672257980345, 0.11869125654733725, 0.09074181092200154, 0.07028452915432115, 0.06949933775306896, 0.0940411160115596, 0.0417967650491398, 0.0966604904525261, 0.04050138487325299, 0.100545537848353, 0.06018146169940399, 0.047688821600905665, 0.053523301803951935, 0.02815140404172732, 0.06788728713774514, 0.10551366130409351, 0.08994045879212315, 0.05215818337229758, 0.027313691674823705, 0.04044908492726003, 0.10809130441562026, 0.09595130081460834, 0.07157385871100232, 0.06653714755323846, 0.09288332899917112, 0.04170913334119024, 0.09844158437931566, 0.0412141315769805, 0.0971215818082098]
train_batch_acc_list = [0.0625, 0.875, 0.890625, 0.96875, 0.921875, 0.859375, 0.953125, 0.953125, 0.96875, 0.953125, 0.9375, 0.9375, 0.953125, 0.96875, 0.96875, 0.96875, 0.9375, 0.96875, 0.953125, 0.953125, 0.953125, 0.953125, 1.0, 0.953125, 0.921875, 0.984375, 0.984375, 0.96875, 0.953125, 0.9375, 0.96875, 0.96875, 0.953125, 0.96875, 0.984375, 0.953125, 1.0, 0.921875, 0.953125, 0.96875, 0.984375, 1.0, 0.953125, 0.953125, 0.984375, 0.984375, 0.96875, 0.96875, 0.953125, 0.96875, 0.96875, 0.953125, 0.96875, 0.984375, 0.96875, 1.0, 0.953125, 0.96875, 0.96875, 0.984375, 1.0, 0.96875, 0.953125, 0.984375, 0.984375, 0.984375, 0.984375, 0.953125, 0.96875, 0.984375, 0.984375, 0.96875, 0.984375, 0.96875, 1.0, 0.953125, 0.984375, 0.96875, 0.984375, 1.0, 0.953125, 0.953125, 0.984375, 0.984375, 0.984375, 0.984375, 0.953125, 0.96875, 0.984375, 0.984375, 0.96875, 0.984375, 0.96875, 1.0, 0.953125, 0.953125, 0.984375, 0.984375, 1.0, 0.984375, 0.96875, 0.984375, 0.984375, 0.984375, 0.984375, 0.953125, 0.96875, 0.984375, 0.984375, 0.96875, 0.984375, 0.96875, 1.0, 0.96875, 0.984375, 0.984375, 0.984375, 1.0, 0.96875, 0.953125, 0.984375, 0.984375, 0.984375, 1.0, 0.953125, 0.96875, 0.984375, 0.984375, 0.96875, 0.984375, 0.96875, 1.0, 0.96875, 0.984375, 0.984375, 0.96875, 1.0, 0.96875, 0.96875, 0.984375, 0.984375, 0.984375, 1.0, 0.953125, 0.96875, 0.984375, 0.984375, 0.96875, 0.984375, 0.96875, 1.0, 0.96875, 1.0, 0.984375, 0.984375, 1.0, 0.984375, 0.96875, 0.984375, 0.984375, 0.984375, 1.0, 0.953125, 0.96875, 0.984375, 0.984375, 0.96875, 0.984375, 0.96875, 1.0, 0.96875, 0.984375, 0.984375, 0.984375, 1.0, 0.984375, 0.96875, 0.984375, 0.984375, 0.984375, 1.0, 0.953125, 0.96875, 0.984375, 0.984375, 0.96875, 0.984375, 0.96875, 1.0, 0.96875]
plt.figure(figsize=(12, 5))
plt.subplot(121)
plt.plot(np.arange(1, len(train_batch_loss_list)+1), train_batch_loss_list)
plt.xlabel('batch_step')
plt.ylabel('train_batch_loss')
plt.legend(['loss'])
plt.subplot(122)
plt.plot(np.arange(1, len(train_batch_acc_list)+1), train_batch_acc_list)
plt.xlabel('batch_step')
plt.ylabel('train_batch_accuracy')
plt.legend(['accuracy'])
plt.savefig('train_batch_loss_acc.png')
plt.show()

train_loss_list = [0.2492875322077296, 0.11972635388529797, 0.09723425890582259, 0.08719722503379598, 0.08129551121036888, 0.07648738203010294, 0.07350172415245854, 0.07156889994933825, 0.07015541003205614, 0.06801979388296468]
test_loss_list = [0.21455528303403293, 0.14104103962051773, 0.1317189717922456, 0.12585157238059164, 0.13590552985220195, 0.12312560888473147, 0.14352308177278172, 0.13344632200638434, 0.13101108695730937, 0.13692955518874717]
train_acc_list = [0.9251833333333334, 0.9652666666666667, 0.9723833333333334, 0.9752833333333333, 0.9770833333333333, 0.97895, 0.98005, 0.98045, 0.98095, 0.9817666666666667]
test_acc_list = [0.9316, 0.9538, 0.9556, 0.959, 0.9562, 0.9586, 0.9514, 0.9552, 0.9555, 0.9548]

plt.figure(figsize=(12, 5))
plt.subplot(121)
plt.plot(np.arange(1, len(train_loss_list)+1), train_loss_list)
plt.plot(np.arange(1, len(test_loss_list)+1), test_loss_list)
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train_loss', 'test_loss'])
plt.subplot(122)
plt.plot(np.arange(1, len(train_acc_list)+1), train_acc_list)
plt.plot(np.arange(1, len(test_acc_list)+1), test_acc_list)
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.legend(['train_acc', 'test_acc'])
plt.show()
plt.tight_layout()
plt.savefig('train_test_loss_acc.png')
